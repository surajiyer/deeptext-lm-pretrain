data_args:
    mlm: True
    train_data_file: "/home/suraj_iyer/lab/DeepText/data/01_raw/20210218_2207_train_nl.txt"
    eval_data_file: "/home/suraj_iyer/lab/DeepText/data/01_raw/20210218_2207_test_nl.txt"
    mlm_probability: .15
    overwrite_cache: False
    block_size: 512

model_args:
    model_name_or_path: GroNLP/bert-base-dutch-cased
    model_type: bert
    config_name:
    tokenizer_name:
    cache_dir:

training_args:
    adam_epsilon: 1e-08
    do_train: True
#     do_eval: True  # becomes True by default when evaluation_strategy is set to anything other than "no"
    evaluation_strategy: epoch
    fp16: False
    fp16_opt_level: '01'
    gradient_accumulation_steps: 16
    learning_rate: 5e-05
    local_rank: -1
    logging_dir: "/home/suraj_iyer/lab/DeepText/data/04_logs"
    logging_first_step: False
    logging_steps: 1000
    max_grad_norm: 1
    no_cuda: False
    num_train_epochs: 3
    output_dir: "/home/suraj_iyer/lab/DeepText/data/03_results"
    overwrite_output_dir: True
    per_device_train_batch_size: 4
    per_device_eval_batch_size: 4
    save_total_limit: 2
    save_steps: 1000
    seed: 42
    tpu_num_cores:
    tpu_metrics_debug: False
    warmup_steps: 0
    weight_decay: 0